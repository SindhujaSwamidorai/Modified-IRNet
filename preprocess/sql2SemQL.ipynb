{"cells":[{"cell_type":"code","source":["\n","from google.colab import drive\n","drive.mount('/content/gdrive')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"9xIxCcrEwKxr","executionInfo":{"status":"ok","timestamp":1673169009429,"user_tz":-330,"elapsed":27157,"user":{"displayName":"Sindhuja S","userId":"00745991774938856916"}},"outputId":"42d06210-6791-4264-c583-1da8b88c974a"},"id":"9xIxCcrEwKxr","execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/gdrive\n"]}]},{"cell_type":"code","source":["cd '/content/gdrive/MyDrive/Colab Notebooks/Modified-IRNet/preprocess'"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"7C3wUQBBwOPK","executionInfo":{"status":"ok","timestamp":1673169013483,"user_tz":-330,"elapsed":557,"user":{"displayName":"Sindhuja S","userId":"00745991774938856916"}},"outputId":"7faf820b-228b-46d1-ff4a-2da5aa309827"},"id":"7C3wUQBBwOPK","execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["/content/gdrive/MyDrive/Colab Notebooks/Modified-IRNet/preprocess\n"]}]},{"cell_type":"code","source":["import nltk\n","nltk.download('averaged_perceptron_tagger')\n","nltk.download('punkt')\n","nltk.download('wordnet')\n","nltk.download('omw-1.4') ##SS: colab"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"pIkwi96SwZDR","executionInfo":{"status":"ok","timestamp":1673169021652,"user_tz":-330,"elapsed":3360,"user":{"displayName":"Sindhuja S","userId":"00745991774938856916"}},"outputId":"5ae6cb28-b7a7-47af-ab30-8cb16e972405"},"id":"pIkwi96SwZDR","execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["[nltk_data] Downloading package averaged_perceptron_tagger to\n","[nltk_data]     /root/nltk_data...\n","[nltk_data]   Unzipping taggers/averaged_perceptron_tagger.zip.\n","[nltk_data] Downloading package punkt to /root/nltk_data...\n","[nltk_data]   Unzipping tokenizers/punkt.zip.\n","[nltk_data] Downloading package wordnet to /root/nltk_data...\n","[nltk_data] Downloading package omw-1.4 to /root/nltk_data...\n"]},{"output_type":"execute_result","data":{"text/plain":["True"]},"metadata":{},"execution_count":3}]},{"cell_type":"code","source":["pip install pattern"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"AGieyH5NwmGa","executionInfo":{"status":"ok","timestamp":1673169053144,"user_tz":-330,"elapsed":25465,"user":{"displayName":"Sindhuja S","userId":"00745991774938856916"}},"outputId":"cb590400-acd2-48e6-951e-bdbbdaa19629"},"id":"AGieyH5NwmGa","execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Collecting pattern\n","  Downloading Pattern-3.6.0.tar.gz (22.2 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m22.2/22.2 MB\u001b[0m \u001b[31m38.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n","Requirement already satisfied: future in /usr/local/lib/python3.8/dist-packages (from pattern) (0.16.0)\n","Collecting backports.csv\n","  Downloading backports.csv-1.0.7-py2.py3-none-any.whl (12 kB)\n","Collecting mysqlclient\n","  Downloading mysqlclient-2.1.1.tar.gz (88 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m88.1/88.1 KB\u001b[0m \u001b[31m9.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n","Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.8/dist-packages (from pattern) (4.6.3)\n","Requirement already satisfied: lxml in /usr/local/lib/python3.8/dist-packages (from pattern) (4.9.2)\n","Collecting feedparser\n","  Downloading feedparser-6.0.10-py3-none-any.whl (81 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m81.1/81.1 KB\u001b[0m \u001b[31m8.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting pdfminer.six\n","  Downloading pdfminer.six-20221105-py3-none-any.whl (5.6 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.6/5.6 MB\u001b[0m \u001b[31m74.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.8/dist-packages (from pattern) (1.21.6)\n","Requirement already satisfied: scipy in /usr/local/lib/python3.8/dist-packages (from pattern) (1.7.3)\n","Requirement already satisfied: nltk in /usr/local/lib/python3.8/dist-packages (from pattern) (3.7)\n","Collecting python-docx\n","  Downloading python-docx-0.8.11.tar.gz (5.6 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.6/5.6 MB\u001b[0m \u001b[31m81.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n","Collecting cherrypy\n","  Downloading CherryPy-18.8.0-py2.py3-none-any.whl (348 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m348.4/348.4 KB\u001b[0m \u001b[31m984.0 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: requests in /usr/local/lib/python3.8/dist-packages (from pattern) (2.25.1)\n","Collecting jaraco.collections\n","  Downloading jaraco.collections-3.8.0-py3-none-any.whl (10 kB)\n","Collecting portend>=2.1.1\n","  Downloading portend-3.1.0-py3-none-any.whl (5.3 kB)\n","Requirement already satisfied: more-itertools in /usr/local/lib/python3.8/dist-packages (from cherrypy->pattern) (9.0.0)\n","Collecting cheroot>=8.2.1\n","  Downloading cheroot-9.0.0-py2.py3-none-any.whl (100 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m100.6/100.6 KB\u001b[0m \u001b[31m11.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting zc.lockfile\n","  Downloading zc.lockfile-2.0-py2.py3-none-any.whl (9.7 kB)\n","Collecting sgmllib3k\n","  Downloading sgmllib3k-1.0.0.tar.gz (5.8 kB)\n","  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n","Requirement already satisfied: tqdm in /usr/local/lib/python3.8/dist-packages (from nltk->pattern) (4.64.1)\n","Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.8/dist-packages (from nltk->pattern) (2022.6.2)\n","Requirement already satisfied: joblib in /usr/local/lib/python3.8/dist-packages (from nltk->pattern) (1.2.0)\n","Requirement already satisfied: click in /usr/local/lib/python3.8/dist-packages (from nltk->pattern) (7.1.2)\n","Requirement already satisfied: charset-normalizer>=2.0.0 in /usr/local/lib/python3.8/dist-packages (from pdfminer.six->pattern) (2.1.1)\n","Collecting cryptography>=36.0.0\n","  Downloading cryptography-39.0.0-cp36-abi3-manylinux_2_24_x86_64.whl (4.1 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.1/4.1 MB\u001b[0m \u001b[31m72.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.8/dist-packages (from requests->pattern) (2.10)\n","Requirement already satisfied: chardet<5,>=3.0.2 in /usr/local/lib/python3.8/dist-packages (from requests->pattern) (4.0.0)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.8/dist-packages (from requests->pattern) (2022.12.7)\n","Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.8/dist-packages (from requests->pattern) (1.24.3)\n","Requirement already satisfied: six>=1.11.0 in /usr/local/lib/python3.8/dist-packages (from cheroot>=8.2.1->cherrypy->pattern) (1.15.0)\n","Collecting jaraco.functools\n","  Downloading jaraco.functools-3.5.2-py3-none-any.whl (7.3 kB)\n","Requirement already satisfied: cffi>=1.12 in /usr/local/lib/python3.8/dist-packages (from cryptography>=36.0.0->pdfminer.six->pattern) (1.15.1)\n","Collecting tempora>=1.8\n","  Downloading tempora-5.2.0-py3-none-any.whl (13 kB)\n","Collecting jaraco.text\n","  Downloading jaraco.text-3.11.0-py3-none-any.whl (11 kB)\n","Collecting jaraco.classes\n","  Downloading jaraco.classes-3.2.3-py3-none-any.whl (6.0 kB)\n","Requirement already satisfied: setuptools in /usr/local/lib/python3.8/dist-packages (from zc.lockfile->cherrypy->pattern) (57.4.0)\n","Requirement already satisfied: pycparser in /usr/local/lib/python3.8/dist-packages (from cffi>=1.12->cryptography>=36.0.0->pdfminer.six->pattern) (2.21)\n","Requirement already satisfied: pytz in /usr/local/lib/python3.8/dist-packages (from tempora>=1.8->portend>=2.1.1->cherrypy->pattern) (2022.7)\n","Requirement already satisfied: inflect in /usr/local/lib/python3.8/dist-packages (from jaraco.text->jaraco.collections->cherrypy->pattern) (2.1.0)\n","Collecting jaraco.context>=4.1\n","  Downloading jaraco.context-4.2.0-py3-none-any.whl (5.0 kB)\n","Requirement already satisfied: importlib-resources in /usr/local/lib/python3.8/dist-packages (from jaraco.text->jaraco.collections->cherrypy->pattern) (5.10.1)\n","Collecting autocommand\n","  Downloading autocommand-2.2.2-py3-none-any.whl (19 kB)\n","Requirement already satisfied: zipp>=3.1.0 in /usr/local/lib/python3.8/dist-packages (from importlib-resources->jaraco.text->jaraco.collections->cherrypy->pattern) (3.11.0)\n","Building wheels for collected packages: pattern, mysqlclient, python-docx, sgmllib3k\n","  Building wheel for pattern (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for pattern: filename=Pattern-3.6-py3-none-any.whl size=22332722 sha256=21e14b9042342cf8ba1b4eb5f39a8c0d6a474d14ef77dd0401cd9a8fbb004da2\n","  Stored in directory: /root/.cache/pip/wheels/ec/ce/8f/bccc2d04f3a25a5a1dd19165b2855ad3203975f25edd5838d6\n","  Building wheel for mysqlclient (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for mysqlclient: filename=mysqlclient-2.1.1-cp38-cp38-linux_x86_64.whl size=102397 sha256=c76daaf9f458177728cef3a5c90a0774a8555dca7aad00e174fad05310c10888\n","  Stored in directory: /root/.cache/pip/wheels/5b/e1/84/a6185eaec318899f59a32d393af7729a0719cd93695d71f9a1\n","  Building wheel for python-docx (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for python-docx: filename=python_docx-0.8.11-py3-none-any.whl size=184505 sha256=ed9e4486515d456131282e84256e9a972badb96e905610b400415f74edbdc47b\n","  Stored in directory: /root/.cache/pip/wheels/32/b8/b2/c4c2b95765e615fe139b0b17b5ea7c0e1b6519b0a9ec8fb34d\n","  Building wheel for sgmllib3k (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for sgmllib3k: filename=sgmllib3k-1.0.0-py3-none-any.whl size=6066 sha256=0bb2dc1aef1105a2bc4c2c05b9fcc1a271d2e7161a070db268551fc33dae0c20\n","  Stored in directory: /root/.cache/pip/wheels/83/63/2f/117884c3b19d46b64d3d61690333aa80c88dc14050e269c546\n","Successfully built pattern mysqlclient python-docx sgmllib3k\n","Installing collected packages: sgmllib3k, backports.csv, zc.lockfile, tempora, python-docx, mysqlclient, jaraco.functools, jaraco.context, jaraco.classes, feedparser, autocommand, portend, jaraco.text, cryptography, cheroot, pdfminer.six, jaraco.collections, cherrypy, pattern\n","Successfully installed autocommand-2.2.2 backports.csv-1.0.7 cheroot-9.0.0 cherrypy-18.8.0 cryptography-39.0.0 feedparser-6.0.10 jaraco.classes-3.2.3 jaraco.collections-3.8.0 jaraco.context-4.2.0 jaraco.functools-3.5.2 jaraco.text-3.11.0 mysqlclient-2.1.1 pattern-3.6 pdfminer.six-20221105 portend-3.1.0 python-docx-0.8.11 sgmllib3k-1.0.0 tempora-5.2.0 zc.lockfile-2.0\n"]}]},{"cell_type":"code","execution_count":null,"id":"1cbb6edd","metadata":{"id":"1cbb6edd"},"outputs":[],"source":["import argparse\n","import json\n","import sys\n","\n","import copy\n","from utils import load_dataSets"]},{"cell_type":"code","execution_count":null,"id":"41fc1b43","metadata":{"id":"41fc1b43"},"outputs":[],"source":["sys.path.append(\"..\")\n","from src.rule.semQL import Root1, Root, N, A, C, T, Sel, Sup, Filter, Order\n","\n","class Parser:\n","    def __init__(self):\n","        self.copy_selec = None\n","        self.sel_result = []\n","        self.colSet = set()\n","\n","    def _init_rule(self):\n","        self.copy_selec = None\n","        self.colSet = set()\n","\n","    def _parse_root(self, sql):\n","        \"\"\"\n","        parsing the sql by the grammar\n","        R ::= Select | Select Filter | Select Order | ... |\n","        :return: [R(), states]\n","        \"\"\"\n","        use_sup, use_ord, use_fil = True, True, False\n","\n","        if sql['sql']['limit'] == None:\n","            use_sup = False\n","\n","        if sql['sql']['orderBy'] == []:\n","            use_ord = False\n","        elif sql['sql']['limit'] != None:\n","            use_ord = False\n","\n","        # check the where and having\n","        if sql['sql']['where'] != [] or \\\n","                        sql['sql']['having'] != []:\n","            use_fil = True\n","\n","        if use_fil and use_sup:\n","            return [Root(0)], ['FILTER', 'SUP', 'SEL']\n","        elif use_fil and use_ord:\n","            return [Root(1)], ['ORDER', 'FILTER', 'SEL']\n","        elif use_sup:\n","            return [Root(2)], ['SUP', 'SEL']\n","        elif use_fil:\n","            return [Root(3)], ['FILTER', 'SEL']\n","        elif use_ord:\n","            return [Root(4)], ['ORDER', 'SEL']\n","        else:\n","            return [Root(5)], ['SEL']\n","\n","    def _parser_column0(self, sql, select):\n","        \"\"\"\n","        Find table of column '*'\n","        :return: T(table_id)\n","        \"\"\"\n","        if len(sql['sql']['from']['table_units']) == 1:\n","            return T(sql['sql']['from']['table_units'][0][1])\n","        else:\n","            table_list = []\n","            for tmp_t in sql['sql']['from']['table_units']:\n","                if type(tmp_t[1]) == int:\n","                    table_list.append(tmp_t[1])\n","            table_set, other_set = set(table_list), set()\n","            for sel_p in select:\n","                if sel_p[1][1][1] != 0:\n","                    other_set.add(sql['col_table'][sel_p[1][1][1]])\n","\n","            if len(sql['sql']['where']) == 1:\n","                other_set.add(sql['col_table'][sql['sql']['where'][0][2][1][1]])\n","            elif len(sql['sql']['where']) == 3:\n","                other_set.add(sql['col_table'][sql['sql']['where'][0][2][1][1]])\n","                other_set.add(sql['col_table'][sql['sql']['where'][2][2][1][1]])\n","            elif len(sql['sql']['where']) == 5:\n","                other_set.add(sql['col_table'][sql['sql']['where'][0][2][1][1]])\n","                other_set.add(sql['col_table'][sql['sql']['where'][2][2][1][1]])\n","                other_set.add(sql['col_table'][sql['sql']['where'][4][2][1][1]])\n","            table_set = table_set - other_set\n","            if len(table_set) == 1:\n","                return T(list(table_set)[0])\n","            elif len(table_set) == 0 and sql['sql']['groupBy'] != []:\n","                return T(sql['col_table'][sql['sql']['groupBy'][0][1]])\n","            else:\n","                question = sql['question']\n","                self.sel_result.append(question)\n","                print('column * table error')\n","                return T(sql['sql']['from']['table_units'][0][1])\n","\n","    def _parse_select(self, sql):\n","        \"\"\"\n","        parsing the sql by the grammar\n","        Select ::= A | AA | AAA | ... |\n","        A ::= agg column table\n","        :return: [Sel(), states]\n","        \"\"\"\n","        result = []\n","        select = sql['sql']['select'][1]\n","        result.append(Sel(0))\n","        result.append(N(len(select) - 1))\n","\n","        for sel in select:\n","            result.append(A(sel[0]))\n","            self.colSet.add(sql['col_set'].index(sql['names'][sel[1][1][1]]))\n","            result.append(C(sql['col_set'].index(sql['names'][sel[1][1][1]])))\n","            # now check for the situation with *\n","            if sel[1][1][1] == 0:\n","                result.append(self._parser_column0(sql, select))\n","            else:\n","                result.append(T(sql['col_table'][sel[1][1][1]]))\n","            if not self.copy_selec:\n","                self.copy_selec = [copy.deepcopy(result[-2]), copy.deepcopy(result[-1])]\n","\n","        return result, None\n","\n","    def _parse_sup(self, sql):\n","        \"\"\"\n","        parsing the sql by the grammar\n","        Sup ::= Most A | Least A\n","        A ::= agg column table\n","        :return: [Sup(), states]\n","        \"\"\"\n","        result = []\n","        select = sql['sql']['select'][1]\n","        if sql['sql']['limit'] == None:\n","            return result, None\n","        if sql['sql']['orderBy'][0] == 'desc':\n","            result.append(Sup(0))\n","        else:\n","            result.append(Sup(1))\n","\n","        result.append(A(sql['sql']['orderBy'][1][0][1][0]))\n","        self.colSet.add(sql['col_set'].index(sql['names'][sql['sql']['orderBy'][1][0][1][1]]))\n","        result.append(C(sql['col_set'].index(sql['names'][sql['sql']['orderBy'][1][0][1][1]])))\n","        if sql['sql']['orderBy'][1][0][1][1] == 0:\n","            result.append(self._parser_column0(sql, select))\n","        else:\n","            result.append(T(sql['col_table'][sql['sql']['orderBy'][1][0][1][1]]))\n","        return result, None\n","\n","    def _parse_filter(self, sql):\n","        \"\"\"\n","        parsing the sql by the grammar\n","        Filter ::= and Filter Filter | ... |\n","        A ::= agg column table\n","        :return: [Filter(), states]\n","        \"\"\"\n","        result = []\n","        # check the where\n","        if sql['sql']['where'] != [] and sql['sql']['having'] != []:\n","            result.append(Filter(0))\n","\n","        if sql['sql']['where'] != []:\n","            # check the not and/or\n","            if len(sql['sql']['where']) == 1:\n","                result.extend(self.parse_one_condition(sql['sql']['where'][0], sql['names'], sql))\n","            elif len(sql['sql']['where']) == 3:\n","                if sql['sql']['where'][1] == 'or':\n","                    result.append(Filter(1))\n","                else:\n","                    result.append(Filter(0))\n","                result.extend(self.parse_one_condition(sql['sql']['where'][0], sql['names'], sql))\n","                result.extend(self.parse_one_condition(sql['sql']['where'][2], sql['names'], sql))\n","            else:\n","                if sql['sql']['where'][1] == 'and' and sql['sql']['where'][3] == 'and':\n","                    result.append(Filter(0))\n","                    result.extend(self.parse_one_condition(sql['sql']['where'][0], sql['names'], sql))\n","                    result.append(Filter(0))\n","                    result.extend(self.parse_one_condition(sql['sql']['where'][2], sql['names'], sql))\n","                    result.extend(self.parse_one_condition(sql['sql']['where'][4], sql['names'], sql))\n","                elif sql['sql']['where'][1] == 'and' and sql['sql']['where'][3] == 'or':\n","                    result.append(Filter(1))\n","                    result.append(Filter(0))\n","                    result.extend(self.parse_one_condition(sql['sql']['where'][0], sql['names'], sql))\n","                    result.extend(self.parse_one_condition(sql['sql']['where'][2], sql['names'], sql))\n","                    result.extend(self.parse_one_condition(sql['sql']['where'][4], sql['names'], sql))\n","                elif sql['sql']['where'][1] == 'or' and sql['sql']['where'][3] == 'and':\n","                    result.append(Filter(1))\n","                    result.append(Filter(0))\n","                    result.extend(self.parse_one_condition(sql['sql']['where'][2], sql['names'], sql))\n","                    result.extend(self.parse_one_condition(sql['sql']['where'][4], sql['names'], sql))\n","                    result.extend(self.parse_one_condition(sql['sql']['where'][0], sql['names'], sql))\n","                else:\n","                    result.append(Filter(1))\n","                    result.append(Filter(1))\n","                    result.extend(self.parse_one_condition(sql['sql']['where'][0], sql['names'], sql))\n","                    result.extend(self.parse_one_condition(sql['sql']['where'][2], sql['names'], sql))\n","                    result.extend(self.parse_one_condition(sql['sql']['where'][4], sql['names'], sql))\n","\n","        # check having\n","        if sql['sql']['having'] != []:\n","            result.extend(self.parse_one_condition(sql['sql']['having'][0], sql['names'], sql))\n","        return result, None\n","\n","    def _parse_order(self, sql):\n","        \"\"\"\n","        parsing the sql by the grammar\n","        Order ::= asc A | desc A\n","        A ::= agg column table\n","        :return: [Order(), states]\n","        \"\"\"\n","        result = []\n","\n","        if 'order' not in sql['query_toks_no_value'] or 'by' not in sql['query_toks_no_value']:\n","            return result, None\n","        elif 'limit' in sql['query_toks_no_value']:\n","            return result, None\n","        else:\n","            if sql['sql']['orderBy'] == []:\n","                return result, None\n","            else:\n","                select = sql['sql']['select'][1]\n","                if sql['sql']['orderBy'][0] == 'desc':\n","                    result.append(Order(0))\n","                else:\n","                    result.append(Order(1))\n","                result.append(A(sql['sql']['orderBy'][1][0][1][0]))\n","                self.colSet.add(sql['col_set'].index(sql['names'][sql['sql']['orderBy'][1][0][1][1]]))\n","                result.append(C(sql['col_set'].index(sql['names'][sql['sql']['orderBy'][1][0][1][1]])))\n","                if sql['sql']['orderBy'][1][0][1][1] == 0:\n","                    result.append(self._parser_column0(sql, select))\n","                else:\n","                    result.append(T(sql['col_table'][sql['sql']['orderBy'][1][0][1][1]]))\n","        return result, None\n","\n","\n","    def parse_one_condition(self, sql_condit, names, sql):\n","        result = []\n","        # check if V(root)\n","        nest_query = True\n","        if type(sql_condit[3]) != dict:\n","            nest_query = False\n","\n","        if sql_condit[0] == True:\n","            if sql_condit[1] == 9:\n","                # not like only with values\n","                fil = Filter(10)\n","            elif sql_condit[1] == 8:\n","                # not in with Root\n","                fil = Filter(19)\n","            else:\n","                print(sql_condit[1])\n","                raise NotImplementedError(\"not implement for the others FIL\")\n","        else:\n","            # check for Filter (<,=,>,!=,between, >=,  <=, ...)\n","            single_map = {1:8,2:2,3:5,4:4,5:7,6:6,7:3}\n","            nested_map = {1:15,2:11,3:13,4:12,5:16,6:17,7:14}\n","            if sql_condit[1] in [1, 2, 3, 4, 5, 6, 7]:\n","                if nest_query == False:\n","                    fil = Filter(single_map[sql_condit[1]])\n","                else:\n","                    fil = Filter(nested_map[sql_condit[1]])\n","            elif sql_condit[1] == 9:\n","                fil = Filter(9)\n","            elif sql_condit[1] == 8:\n","                fil = Filter(18)\n","            else:\n","                print(sql_condit[1])\n","                raise NotImplementedError(\"not implement for the others FIL\")\n","\n","        result.append(fil)\n","        result.append(A(sql_condit[2][1][0]))\n","        self.colSet.add(sql['col_set'].index(sql['names'][sql_condit[2][1][1]]))\n","        result.append(C(sql['col_set'].index(sql['names'][sql_condit[2][1][1]])))\n","        if sql_condit[2][1][1] == 0:\n","            select = sql['sql']['select'][1]\n","            result.append(self._parser_column0(sql, select))\n","        else:\n","            result.append(T(sql['col_table'][sql_condit[2][1][1]]))\n","\n","        # check for the nested value\n","        if type(sql_condit[3]) == dict:\n","            nest_query = {}\n","            nest_query['names'] = names\n","            nest_query['query_toks_no_value'] = \"\"\n","            nest_query['sql'] = sql_condit[3]\n","            nest_query['col_table'] = sql['col_table']\n","            nest_query['col_set'] = sql['col_set']\n","            nest_query['table_names'] = sql['table_names']\n","            nest_query['question'] = sql['question']\n","            nest_query['query'] = sql['query']\n","            nest_query['keys'] = sql['keys']\n","            result.extend(self.parser(nest_query))\n","\n","        return result\n","\n","    def _parse_step(self, state, sql):\n","\n","        if state == 'ROOT':\n","            return self._parse_root(sql)\n","\n","        if state == 'SEL':\n","            return self._parse_select(sql)\n","\n","        elif state == 'SUP':\n","            return self._parse_sup(sql)\n","\n","        elif state == 'FILTER':\n","            return self._parse_filter(sql)\n","\n","        elif state == 'ORDER':\n","            return self._parse_order(sql)\n","        else:\n","            raise NotImplementedError(\"Not the right state\")\n","\n","    def full_parse(self, query):\n","        sql = query['sql']\n","        nest_query = {}\n","        nest_query['names'] = query['names']\n","        nest_query['query_toks_no_value'] = \"\"\n","        nest_query['col_table'] = query['col_table']\n","        nest_query['col_set'] = query['col_set']\n","        nest_query['table_names'] = query['table_names']\n","        nest_query['question'] = query['question']\n","        nest_query['query'] = query['query']\n","        nest_query['keys'] = query['keys']\n","\n","        if sql['intersect']:\n","            results = [Root1(0)]\n","            nest_query['sql'] = sql['intersect']\n","            results.extend(self.parser(query))\n","            results.extend(self.parser(nest_query))\n","            return results\n","\n","        if sql['union']:\n","            results = [Root1(1)]\n","            nest_query['sql'] = sql['union']\n","            results.extend(self.parser(query))\n","            results.extend(self.parser(nest_query))\n","            return results\n","\n","        if sql['except']:\n","            results = [Root1(2)]\n","            nest_query['sql'] = sql['except']\n","            results.extend(self.parser(query))\n","            results.extend(self.parser(nest_query))\n","            return results\n","\n","        results = [Root1(3)]\n","        results.extend(self.parser(query))\n","\n","        return results\n","\n","    def parser(self, query):\n","        stack = [\"ROOT\"]\n","        result = []\n","        while len(stack) > 0:\n","            state = stack.pop()\n","            step_result, step_state = self._parse_step(state, query)\n","            result.extend(step_result)\n","            if step_state:\n","                stack.extend(step_state)\n","        return result\n","\n"]},{"cell_type":"code","execution_count":null,"id":"25515510","metadata":{"id":"25515510"},"outputs":[],"source":["## arguments\n","class args:\n","    data_path = '../data/process_data_dev_nested.json'\n","    table_path = '../data/tables.json'\n","    output = '../data/semQL_data_dev_nested.json'\n","    conceptNet = '../data'\n"]},{"cell_type":"code","execution_count":null,"id":"f68956cf","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":400},"id":"f68956cf","executionInfo":{"status":"ok","timestamp":1673169155577,"user_tz":-330,"elapsed":2357,"user":{"displayName":"Sindhuja S","userId":"00745991774938856916"}},"outputId":"d95e57dd-2125-473c-db48-3379935b8de7"},"outputs":[{"output_type":"stream","name":"stdout","text":["column * table error\n","column * table error\n","column * table error\n","column * table error\n","column * table error\n","column * table error\n","column * table error\n","column * table error\n","column * table error\n","column * table error\n","column * table error\n","column * table error\n","column * table error\n","column * table error\n","column * table error\n","column * table error\n","column * table error\n","column * table error\n","Finished 1032 datas and failed 2 datas\n"]},{"output_type":"execute_result","data":{"text/plain":["\" Oversampling \\ndata = pd.DataFrame(processed_data)\\nrandOverSamplr = RandomOverSampler(sampling_strategy={1:data.shape[0]//4}, random_state=777)\\nnew_data, new_Y = randOverSamplr.fit_resample(data,data['nested'])\\n#new_data.drop('nested', axis=1, inplace=True)\\n\\nwith open(args.output, 'w', encoding='utf8') as f:\\n    #f.write(json.dumps(processed_data))\\n    json.dump(list(new_data.to_dict(orient='index').values()), f)\\n\""],"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"}},"metadata":{},"execution_count":8}],"source":["import pandas as pd\n","from imblearn.over_sampling import RandomOverSampler\n","\n","parser = Parser()\n","\n","# loading dataSets\n","datas, table = load_dataSets(args)\n","processed_data = []\n","\n","for i, d in enumerate(datas):\n","    if len(datas[i]['sql']['select'][1]) > 5:\n","        continue\n","    r = parser.full_parse(datas[i])\n","    datas[i]['rule_label'] = \" \".join([str(x) for x in r])\n","    processed_data.append(datas[i])\n","\n","print('Finished %s datas and failed %s datas' % (len(processed_data), len(datas) - len(processed_data)))\n","with open(args.output, 'w', encoding='utf8') as f:\n","    f.write(json.dumps(processed_data))\n","\n","''' Oversampling\n","data = pd.DataFrame(processed_data)\n","randOverSamplr = RandomOverSampler(sampling_strategy={1:data.shape[0]//4}, random_state=777)\n","new_data, new_Y = randOverSamplr.fit_resample(data,data['nested'])\n","#new_data.drop('nested', axis=1, inplace=True)\n","\n","with open(args.output, 'w', encoding='utf8') as f:\n","    #f.write(json.dumps(processed_data))\n","    json.dump(list(new_data.to_dict(orient='index').values()), f)\n","'''"]},{"cell_type":"code","source":["\n","print('Total processed data after random over sampling: ', len(new_data))"],"metadata":{"id":"25rUD09dxCss","executionInfo":{"status":"ok","timestamp":1667198367993,"user_tz":-330,"elapsed":512,"user":{"displayName":"Sindhuja S","userId":"00745991774938856916"}},"outputId":"1ceef15a-185f-401e-b166-3df3022feac6","colab":{"base_uri":"https://localhost:8080/"}},"id":"25rUD09dxCss","execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Total processed data after random over sampling:  7731\n"]}]},{"cell_type":"code","execution_count":null,"id":"a8019b1f","metadata":{"id":"a8019b1f"},"outputs":[],"source":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.8"},"colab":{"provenance":[]}},"nbformat":4,"nbformat_minor":5}