{"cells":[{"cell_type":"code","source":["\n","from google.colab import drive\n","drive.mount('/content/gdrive')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"DwZVZJEsvcP_","executionInfo":{"status":"ok","timestamp":1667197969989,"user_tz":-330,"elapsed":21155,"user":{"displayName":"Sindhuja S","userId":"00745991774938856916"}},"outputId":"28358b22-1cb8-40f8-e021-bba475c3a3f7"},"id":"DwZVZJEsvcP_","execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/gdrive\n"]}]},{"cell_type":"code","source":["cd '/content/gdrive/MyDrive/Colab Notebooks/Modified-IRNet/preprocess'"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"-H-h5Lutvg9V","executionInfo":{"status":"ok","timestamp":1667197981738,"user_tz":-330,"elapsed":325,"user":{"displayName":"Sindhuja S","userId":"00745991774938856916"}},"outputId":"9f280ddc-0b86-493d-ee8e-b397ac6891d6"},"id":"-H-h5Lutvg9V","execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["/content/gdrive/MyDrive/Colab Notebooks/Modified-IRNet/preprocess\n"]}]},{"cell_type":"code","execution_count":null,"id":"f6d1801f","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"f6d1801f","executionInfo":{"status":"ok","timestamp":1667198004871,"user_tz":-330,"elapsed":741,"user":{"displayName":"Sindhuja S","userId":"00745991774938856916"}},"outputId":"1436bf34-f313-45a6-8846-ec3366f2cd83"},"outputs":[{"output_type":"stream","name":"stderr","text":["[nltk_data] Downloading package averaged_perceptron_tagger to\n","[nltk_data]     /root/nltk_data...\n","[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n","[nltk_data]       date!\n","[nltk_data] Downloading package punkt to /root/nltk_data...\n","[nltk_data]   Package punkt is already up-to-date!\n","[nltk_data] Downloading package wordnet to /root/nltk_data...\n","[nltk_data]   Package wordnet is already up-to-date!\n","[nltk_data] Downloading package omw-1.4 to /root/nltk_data...\n"]},{"output_type":"execute_result","data":{"text/plain":["True"]},"metadata":{},"execution_count":6}],"source":["import nltk\n","nltk.download('averaged_perceptron_tagger')\n","nltk.download('punkt')\n","nltk.download('wordnet')\n","nltk.download('omw-1.4') ##SS: colab"]},{"cell_type":"code","execution_count":null,"id":"f1dd1a71","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"f1dd1a71","executionInfo":{"status":"ok","timestamp":1667198037153,"user_tz":-330,"elapsed":26027,"user":{"displayName":"Sindhuja S","userId":"00745991774938856916"}},"outputId":"d9cdb245-fd1a-4c12-a892-681088cec02d"},"outputs":[{"output_type":"stream","name":"stdout","text":["Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Collecting pattern\n","  Downloading Pattern-3.6.0.tar.gz (22.2 MB)\n","\u001b[K     |████████████████████████████████| 22.2 MB 54.7 MB/s \n","\u001b[?25hRequirement already satisfied: future in /usr/local/lib/python3.7/dist-packages (from pattern) (0.16.0)\n","Collecting backports.csv\n","  Downloading backports.csv-1.0.7-py2.py3-none-any.whl (12 kB)\n","Collecting mysqlclient\n","  Downloading mysqlclient-2.1.1.tar.gz (88 kB)\n","\u001b[K     |████████████████████████████████| 88 kB 6.5 MB/s \n","\u001b[?25hRequirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.7/dist-packages (from pattern) (4.6.3)\n","Requirement already satisfied: lxml in /usr/local/lib/python3.7/dist-packages (from pattern) (4.9.1)\n","Collecting feedparser\n","  Downloading feedparser-6.0.10-py3-none-any.whl (81 kB)\n","\u001b[K     |████████████████████████████████| 81 kB 8.7 MB/s \n","\u001b[?25hCollecting pdfminer.six\n","  Downloading pdfminer.six-20220524-py3-none-any.whl (5.6 MB)\n","\u001b[K     |████████████████████████████████| 5.6 MB 31.4 MB/s \n","\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from pattern) (1.21.6)\n","Requirement already satisfied: scipy in /usr/local/lib/python3.7/dist-packages (from pattern) (1.7.3)\n","Requirement already satisfied: nltk in /usr/local/lib/python3.7/dist-packages (from pattern) (3.7)\n","Collecting python-docx\n","  Downloading python-docx-0.8.11.tar.gz (5.6 MB)\n","\u001b[K     |████████████████████████████████| 5.6 MB 43.2 MB/s \n","\u001b[?25hCollecting cherrypy\n","  Downloading CherryPy-18.8.0-py2.py3-none-any.whl (348 kB)\n","\u001b[K     |████████████████████████████████| 348 kB 53.4 MB/s \n","\u001b[?25hRequirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from pattern) (2.23.0)\n","Collecting portend>=2.1.1\n","  Downloading portend-3.1.0-py3-none-any.whl (5.3 kB)\n","Requirement already satisfied: more-itertools in /usr/local/lib/python3.7/dist-packages (from cherrypy->pattern) (9.0.0)\n","Collecting cheroot>=8.2.1\n","  Downloading cheroot-8.6.0-py2.py3-none-any.whl (104 kB)\n","\u001b[K     |████████████████████████████████| 104 kB 55.1 MB/s \n","\u001b[?25hCollecting zc.lockfile\n","  Downloading zc.lockfile-2.0-py2.py3-none-any.whl (9.7 kB)\n","Collecting jaraco.collections\n","  Downloading jaraco.collections-3.7.0-py3-none-any.whl (10 kB)\n","Collecting jaraco.functools\n","  Downloading jaraco.functools-3.5.2-py3-none-any.whl (7.3 kB)\n","Requirement already satisfied: six>=1.11.0 in /usr/local/lib/python3.7/dist-packages (from cheroot>=8.2.1->cherrypy->pattern) (1.15.0)\n","Collecting tempora>=1.8\n","  Downloading tempora-5.0.2-py3-none-any.whl (15 kB)\n","Requirement already satisfied: pytz in /usr/local/lib/python3.7/dist-packages (from tempora>=1.8->portend>=2.1.1->cherrypy->pattern) (2022.5)\n","Collecting sgmllib3k\n","  Downloading sgmllib3k-1.0.0.tar.gz (5.8 kB)\n","Collecting jaraco.text\n","  Downloading jaraco.text-3.10.0-py3-none-any.whl (11 kB)\n","Collecting jaraco.classes\n","  Downloading jaraco.classes-3.2.3-py3-none-any.whl (6.0 kB)\n","Requirement already satisfied: inflect in /usr/local/lib/python3.7/dist-packages (from jaraco.text->jaraco.collections->cherrypy->pattern) (2.1.0)\n","Requirement already satisfied: importlib-resources in /usr/local/lib/python3.7/dist-packages (from jaraco.text->jaraco.collections->cherrypy->pattern) (5.10.0)\n","Collecting jaraco.context>=4.1\n","  Downloading jaraco.context-4.1.2-py3-none-any.whl (4.7 kB)\n","Collecting autocommand\n","  Downloading autocommand-2.2.1-py3-none-any.whl (22 kB)\n","Requirement already satisfied: zipp>=3.1.0 in /usr/local/lib/python3.7/dist-packages (from importlib-resources->jaraco.text->jaraco.collections->cherrypy->pattern) (3.9.0)\n","Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from nltk->pattern) (1.2.0)\n","Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from nltk->pattern) (4.64.1)\n","Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from nltk->pattern) (7.1.2)\n","Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.7/dist-packages (from nltk->pattern) (2022.6.2)\n","Collecting cryptography>=36.0.0\n","  Downloading cryptography-38.0.1-cp36-abi3-manylinux_2_24_x86_64.whl (4.0 MB)\n","\u001b[K     |████████████████████████████████| 4.0 MB 43.1 MB/s \n","\u001b[?25hRequirement already satisfied: charset-normalizer>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from pdfminer.six->pattern) (2.1.1)\n","Requirement already satisfied: cffi>=1.12 in /usr/local/lib/python3.7/dist-packages (from cryptography>=36.0.0->pdfminer.six->pattern) (1.15.1)\n","Requirement already satisfied: pycparser in /usr/local/lib/python3.7/dist-packages (from cffi>=1.12->cryptography>=36.0.0->pdfminer.six->pattern) (2.21)\n","Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->pattern) (1.24.3)\n","Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->pattern) (3.0.4)\n","Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->pattern) (2.10)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->pattern) (2022.9.24)\n","Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from zc.lockfile->cherrypy->pattern) (57.4.0)\n","Building wheels for collected packages: pattern, mysqlclient, python-docx, sgmllib3k\n","  Building wheel for pattern (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for pattern: filename=Pattern-3.6-py3-none-any.whl size=22332721 sha256=9e3d34d11f77b11dc3f89a35d2b459e8faf56771ad9852ecda10bd2de3af2a7b\n","  Stored in directory: /root/.cache/pip/wheels/8d/1f/4e/9b67afd2430d55dee90bd57618dd7d899f1323e5852c465682\n","  Building wheel for mysqlclient (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for mysqlclient: filename=mysqlclient-2.1.1-cp37-cp37m-linux_x86_64.whl size=100002 sha256=b9ced27bd0ee1a8d920f14440b3ee2bfe8d02f7a0a8d61a27022bf5dbbe19039\n","  Stored in directory: /root/.cache/pip/wheels/95/2d/67/2cb3f82e435fc8e055cb2761a15a0812bf086068f6fb835462\n","  Building wheel for python-docx (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for python-docx: filename=python_docx-0.8.11-py3-none-any.whl size=184507 sha256=f2515f10e5e3034230a2cbf9e20bf80e57a0a7f063bcbbe456b47975c65a76d5\n","  Stored in directory: /root/.cache/pip/wheels/f6/6f/b9/d798122a8b55b74ad30b5f52b01482169b445fbb84a11797a6\n","  Building wheel for sgmllib3k (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for sgmllib3k: filename=sgmllib3k-1.0.0-py3-none-any.whl size=6066 sha256=6fb5471cc71acdf5b2835ff5450fd9acd379bd86ecd88f9ddd695e77e074e59f\n","  Stored in directory: /root/.cache/pip/wheels/73/ad/a4/0dff4a6ef231fc0dfa12ffbac2a36cebfdddfe059f50e019aa\n","Successfully built pattern mysqlclient python-docx sgmllib3k\n","Installing collected packages: jaraco.functools, jaraco.context, autocommand, tempora, jaraco.text, jaraco.classes, zc.lockfile, sgmllib3k, portend, jaraco.collections, cryptography, cheroot, python-docx, pdfminer.six, mysqlclient, feedparser, cherrypy, backports.csv, pattern\n","Successfully installed autocommand-2.2.1 backports.csv-1.0.7 cheroot-8.6.0 cherrypy-18.8.0 cryptography-38.0.1 feedparser-6.0.10 jaraco.classes-3.2.3 jaraco.collections-3.7.0 jaraco.context-4.1.2 jaraco.functools-3.5.2 jaraco.text-3.10.0 mysqlclient-2.1.1 pattern-3.6 pdfminer.six-20220524 portend-3.1.0 python-docx-0.8.11 sgmllib3k-1.0.0 tempora-5.0.2 zc.lockfile-2.0\n"]}],"source":["!pip install pattern"]},{"cell_type":"code","execution_count":null,"id":"4e4af847","metadata":{"id":"4e4af847"},"outputs":[],"source":["import json\n","import argparse\n","import nltk\n","import os\n","import pickle\n","from utils import symbol_filter, re_lemma, fully_part_header, group_header, partial_header, num2year, group_symbol, group_values, group_digital\n","from utils import AGG, wordnet_lemmatizer\n","from utils import load_dataSets"]},{"cell_type":"code","execution_count":null,"id":"36146191","metadata":{"id":"36146191"},"outputs":[],"source":["def process_datas(datas, args):\n","    \"\"\"\n","\n","    :param datas:\n","    :param args:\n","    :return:\n","    \"\"\"\n","    with open(os.path.join(args.conceptNet, 'english_RelatedTo.pkl'), 'rb') as f:\n","        english_RelatedTo = pickle.load(f)\n","\n","    with open(os.path.join(args.conceptNet, 'english_IsA.pkl'), 'rb') as f:\n","        english_IsA = pickle.load(f)\n","\n","    # copy of the origin question_toks\n","    for d in datas:\n","        if 'origin_question_toks' not in d:\n","            d['origin_question_toks'] = d['question_toks']\n","\n","    for entry in datas:\n","        entry['question_toks'] = symbol_filter(entry['question_toks'])\n","        origin_question_toks = symbol_filter([x for x in entry['origin_question_toks'] if x.lower() != 'the'])\n","        question_toks = [wordnet_lemmatizer.lemmatize(x.lower()) for x in entry['question_toks'] if x.lower() != 'the']\n","\n","        entry['question_toks'] = question_toks\n","\n","        table_names = []\n","        table_names_pattern = []\n","\n","        for y in entry['table_names']:\n","            x = [wordnet_lemmatizer.lemmatize(x.lower()) for x in y.split(' ')]\n","            table_names.append(\" \".join(x))\n","            x = [re_lemma(x.lower()) for x in y.split(' ')]\n","            table_names_pattern.append(\" \".join(x))\n","\n","        header_toks = []\n","        header_toks_list = []\n","\n","        header_toks_pattern = []\n","        header_toks_list_pattern = []\n","\n","        for y in entry['col_set']:\n","            x = [wordnet_lemmatizer.lemmatize(x.lower()) for x in y.split(' ')]\n","            header_toks.append(\" \".join(x))\n","            header_toks_list.append(x)\n","\n","            x = [re_lemma(x.lower()) for x in y.split(' ')]\n","            header_toks_pattern.append(\" \".join(x))\n","            header_toks_list_pattern.append(x)\n","\n","        num_toks = len(question_toks)\n","        idx = 0\n","        tok_concol = []\n","        type_concol = []\n","        nltk_result = nltk.pos_tag(question_toks)\n","\n","        while idx < num_toks:\n","\n","            # fully header\n","            end_idx, header = fully_part_header(question_toks, idx, num_toks, header_toks)\n","            if header:\n","                tok_concol.append(question_toks[idx: end_idx])\n","                type_concol.append([\"col\"])\n","                idx = end_idx\n","                continue\n","\n","            # check for table\n","            end_idx, tname = group_header(question_toks, idx, num_toks, table_names)\n","            if tname:\n","                tok_concol.append(question_toks[idx: end_idx])\n","                type_concol.append([\"table\"])\n","                idx = end_idx\n","                continue\n","\n","            # check for column\n","            end_idx, header = group_header(question_toks, idx, num_toks, header_toks)\n","            if header:\n","                tok_concol.append(question_toks[idx: end_idx])\n","                type_concol.append([\"col\"])\n","                idx = end_idx\n","                continue\n","\n","            # check for partial column\n","            end_idx, tname = partial_header(question_toks, idx, header_toks_list)\n","            if tname:\n","                tok_concol.append(tname)\n","                type_concol.append([\"col\"])\n","                idx = end_idx\n","                continue\n","\n","            # check for aggregation\n","            end_idx, agg = group_header(question_toks, idx, num_toks, AGG)\n","            if agg:\n","                tok_concol.append(question_toks[idx: end_idx])\n","                type_concol.append([\"agg\"])\n","                idx = end_idx\n","                continue\n","\n","            if nltk_result[idx][1] == 'RBR' or nltk_result[idx][1] == 'JJR':\n","                tok_concol.append([question_toks[idx]])\n","                type_concol.append(['MORE'])\n","                idx += 1\n","                continue\n","\n","            if nltk_result[idx][1] == 'RBS' or nltk_result[idx][1] == 'JJS':\n","                tok_concol.append([question_toks[idx]])\n","                type_concol.append(['MOST'])\n","                idx += 1\n","                continue\n","\n","            # string match for Time Format\n","            if num2year(question_toks[idx]):\n","                question_toks[idx] = 'year'\n","                end_idx, header = group_header(question_toks, idx, num_toks, header_toks)\n","                if header:\n","                    tok_concol.append(question_toks[idx: end_idx])\n","                    type_concol.append([\"col\"])\n","                    idx = end_idx\n","                    continue\n","\n","            def get_concept_result(toks, graph):\n","                for begin_id in range(0, len(toks)):\n","                    for r_ind in reversed(range(1, len(toks) + 1 - begin_id)):\n","                        tmp_query = \"_\".join(toks[begin_id:r_ind])\n","                        if tmp_query in graph:\n","                            mi = graph[tmp_query]\n","                            for col in entry['col_set']:\n","                                if col in mi:\n","                                    return col\n","\n","            end_idx, symbol = group_symbol(question_toks, idx, num_toks)\n","            if symbol:\n","                tmp_toks = [x for x in question_toks[idx: end_idx]]\n","                assert len(tmp_toks) > 0, print(symbol, question_toks)\n","                pro_result = get_concept_result(tmp_toks, english_IsA)\n","                if pro_result is None:\n","                    pro_result = get_concept_result(tmp_toks, english_RelatedTo)\n","                if pro_result is None:\n","                    pro_result = \"NONE\"\n","                for tmp in tmp_toks:\n","                    tok_concol.append([tmp])\n","                    type_concol.append([pro_result])\n","                    pro_result = \"NONE\"\n","                idx = end_idx\n","                continue\n","\n","            end_idx, values = group_values(origin_question_toks, idx, num_toks)\n","            if values and (len(values) > 1 or question_toks[idx - 1] not in ['?', '.']):\n","                tmp_toks = [wordnet_lemmatizer.lemmatize(x) for x in question_toks[idx: end_idx] if x.isalnum() is True]\n","                assert len(tmp_toks) > 0, print(question_toks[idx: end_idx], values, question_toks, idx, end_idx)\n","                pro_result = get_concept_result(tmp_toks, english_IsA)\n","                if pro_result is None:\n","                    pro_result = get_concept_result(tmp_toks, english_RelatedTo)\n","                if pro_result is None:\n","                    pro_result = \"NONE\"\n","                for tmp in tmp_toks:\n","                    tok_concol.append([tmp])\n","                    type_concol.append([pro_result])\n","                    pro_result = \"NONE\"\n","                idx = end_idx\n","                continue\n","\n","            result = group_digital(question_toks, idx)\n","            if result is True:\n","                tok_concol.append(question_toks[idx: idx + 1])\n","                type_concol.append([\"value\"])\n","                idx += 1\n","                continue\n","            if question_toks[idx] == ['ha']:\n","                question_toks[idx] = ['have']\n","\n","            tok_concol.append([question_toks[idx]])\n","            type_concol.append(['NONE'])\n","            idx += 1\n","            continue\n","\n","        entry['question_arg'] = tok_concol\n","        entry['question_arg_type'] = type_concol\n","        entry['nltk_pos'] = nltk_result\n","        entry['nested'] = 1 if entry['query'].count('SELECT') > 1 else 0\n","\n","    return datas\n"]},{"cell_type":"code","execution_count":null,"id":"e58ae8d1","metadata":{"id":"e58ae8d1"},"outputs":[],"source":["## arguments\n","class args:\n","    data_path = '../data/train_spider.json'\n","    table_path = '../data/tables.json'\n","    output = '../data/process_data_train.json'\n","    conceptNet = '../data'\n"]},{"cell_type":"code","execution_count":null,"id":"a8c6c508","metadata":{"id":"a8c6c508"},"outputs":[],"source":["# loading dataSets\n","datas, table = load_dataSets(args)\n","\n"]},{"cell_type":"code","execution_count":null,"id":"80c15e84","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"80c15e84","executionInfo":{"status":"ok","timestamp":1667198111231,"user_tz":-330,"elapsed":23896,"user":{"displayName":"Sindhuja S","userId":"00745991774938856916"}},"outputId":"04fd61d8-0315-4c6a-fe41-2435480d9e41"},"outputs":[{"output_type":"stream","name":"stdout","text":["<_io.TextIOWrapper name='../data/process_data_train.json' mode='w' encoding='UTF-8'>\n"]}],"source":["# conceptnet downloaded from worksheets.codalab.org\n","# process datasets\n","#import pandas as pd\n","#from imblearn.over_sampling import RandomOverSampler\n","process_result = process_datas(datas, args)\n","#data = pd.DataFrame(datas)\n","#randOverSamplr = RandomOverSampler(sampling_strategy={1:data.shape[0]//4}, random_state=777)\n","#new_data, new_Y = randOverSamplr.fit_resample(data,data['nested'])\n","#new_data.drop('nested', axis=1, inplace=True)\n","with open(args.output, 'w') as f:\n","    print(f)\n","    #json.dump(list(new_data.to_dict(orient='index').values()), f)\n","    json.dump(datas, f)\n"]},{"cell_type":"code","execution_count":null,"id":"373be278","metadata":{"id":"373be278"},"outputs":[],"source":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.8"},"colab":{"provenance":[]},"accelerator":"TPU","gpuClass":"standard"},"nbformat":4,"nbformat_minor":5}